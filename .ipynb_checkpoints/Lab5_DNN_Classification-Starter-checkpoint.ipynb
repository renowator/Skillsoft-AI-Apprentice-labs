{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Apprentice Lab 5 Starter Code\n",
    "#### Deep Neural Network Classification\n",
    "\n",
    "1. Load dataset provided into Python described below\n",
    "2. Use the provided normalize_column function to normalize values\n",
    "3. Separate training and testing data\n",
    "4. Construct a Deep Neural Network model in Keras with several hidden layers named DNNmodel\n",
    "5. Train DNNmodel on training data\n",
    "6. Evaluate results\n",
    "\n",
    "Resources:\n",
    "\n",
    "https://keras.io/guides/sequential_model/\n",
    "\n",
    "https://keras.io/api/layers/core_layers/dense/\n",
    "\n",
    "https://keras.io/api/layers/core_layers/input/\n",
    "\n",
    "https://keras.io/api/layers/activations/\n",
    "\n",
    "https://keras.io/api/losses/probabilistic_losses/#categoricalcrossentropy-class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#       IMPORT REQUIRED LIBRARIES\n",
    "import pandas\n",
    "import numpy as np\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Load the data into Python file: sensor_readings_24.data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset information\n",
    "\n",
    "1. Title of Database: Wall-Following navigation task with mobile robot SCITOS-G5\n",
    "\n",
    "2. Sources:\n",
    "   (a) Creators: \tAnanda Freire, Marcus Veloso and Guilherme Barreto\n",
    "\t\t\tDepartment of Teleinformatics Engineering\n",
    "\t\t\tFederal University of Ceará\n",
    "\t\t\tFortaleza, Ceará, Brazil\n",
    "\n",
    "   (b) Donors of database: Ananda Freire (anandalf@gmail.com)\n",
    "\t\t\t   Guilherme Barreto (guilherme@deti.ufc.br)\t\n",
    "\t  \n",
    "   (c) Date received: August, 2010\n",
    "\n",
    "3. Past Usage:\n",
    "   (a) \tAnanda L. Freire, Guilherme A. Barreto, Marcus Veloso and Antonio T. Varela (2009),\n",
    "\t\"Short-Term Memory Mechanisms in Neural Network Learning of Robot Navigation\n",
    "\tTasks: A Case Study\". Proceedings of the 6th Latin American Robotics Symposium (LARS'2009),\n",
    "\tValparaíso-Chile, pages 1-6, DOI: 10.1109/LARS.2009.5418323 \n",
    "\n",
    "4. Relevant Information Paragraph:\n",
    "   -- \tThe data were collected as the SCITOS G5 navigates through the room following the wall in a clockwise\n",
    "\tdirection, for 4 rounds. To navigate, the robot uses 24 ultrasound sensors arranged circularly around its \"waist\". \n",
    "\tThe numbering of the ultrasound sensors starts at the front of the robot and increases in clockwise direction.\n",
    "\n",
    "   -- \tThe provided files comprise three diferent data sets. The first one contains the raw values of the measurements \n",
    "\tof all 24 ultrasound sensors and the corresponding class label (see Section 7). Sensor readings are sampled at a \n",
    "\trate of 9 samples per second.\n",
    "\n",
    "\tThe second one contains four sensor readings named 'simplified distances' and the corresponding class label (see Section 7). \n",
    "\tThese simplified distances are referred to as the 'front distance', 'left distance', 'right distance' and 'back distance'. \n",
    "\tThey consist, respectively, of the minimum sensor readings among those within 60 degree arcs located at the front, left, \n",
    "\tright and back parts of the robot.\n",
    "\n",
    "\tThe third one contains only the front and left simplified distances and the corresponding class label (see Section 7). \n",
    "\t\n",
    "   --   It is worth mentioning that the 24 ultrasound readings and the simplified distances were collected at the same \t\t\n",
    "\ttime step, so each file has the same number of rows (one for each sampling time step).                                                           \n",
    "\n",
    "   --   The wall-following task and data gathering were designed to test the hypothesis that this apparently simple navigation task \n",
    "\tis indeed a non-linearly separable classification task. Thus, linear classifiers, such as the Perceptron network, are not able \n",
    "\tto learn the task and command the robot around the room without collisions. Nonlinear neural classifiers, such as the MLP network, \n",
    "\tare able to learn the task and command the robot successfully without collisions. \n",
    "\n",
    "   --   If some kind of short-term memory mechanism is provided to the neural classifiers, their performances are improved in general. \n",
    "\tFor example, if past inputs are provided together with current sensor readings, even the Perceptron becomes able to \n",
    "\tlearn the task and command the robot succesfully. If a recurrent neural network, such as the Elman network, is used to \n",
    "\tlearn the task, the resulting dynamical classifier is able to learn the task using less hidden neurons than the MLP network.\n",
    "\n",
    "   --   Files with different number of sensor readings were built in order to evaluate the performance of the classifiers \n",
    "\twith respect to the number of inputs.\n",
    "\n",
    "5. Number of Instances: 5456\n",
    "\n",
    "6. Number of Attributes \n",
    "   -- sensor_readings_24.data: 24 numeric attributes and the class.\n",
    "   -- sensor_readings_4.data:   4 numeric attributes and the class.\n",
    "   -- sensor_readings_2.data:   2 numeric attributes and the class.\n",
    "\n",
    "7. For Each Attribute: \n",
    "   -- File sensor_readings_24.data:\n",
    "\t 1. US1: ultrasound sensor at the front of the robot (reference angle: 180°) - (numeric: real)\n",
    "\t 2. US2: ultrasound reading (reference angle: -165°) - (numeric: real)\n",
    "\t 3. US3: ultrasound reading (reference angle: -150°) - (numeric: real)\n",
    "\t 4. US4: ultrasound reading (reference angle: -135°) - (numeric: real)\n",
    "\t 5. US5: ultrasound reading (reference angle: -120°) - (numeric: real)\n",
    "\t 6. US6: ultrasound reading (reference angle: -105°) - (numeric: real)\n",
    "\t 7. US7: ultrasound reading (reference angle: -90°) - (numeric: real)\n",
    "\t 8. US8: ultrasound reading (reference angle: -75°) - (numeric: real)\n",
    "\t 9. US9: ultrasound reading (reference angle: -60°) - (numeric: real)\n",
    "\t10. US10: ultrasound reading (reference angle: -45°) - (numeric: real)\n",
    "\t11. US11: ultrasound reading (reference angle: -30°) - (numeric: real)\n",
    "\t12. US12: ultrasound reading (reference angle: -15°) - (numeric: real)\n",
    "\t13. US13: reading of ultrasound sensor situated at the back of the robot (reference angle: 0°) - (numeric: real)\n",
    "\t14. US14: ultrasound reading (reference angle: 15°) - (numeric: real)\n",
    "\t15. US15: ultrasound reading (reference angle: 30°) - (numeric: real)\n",
    "\t16. US16: ultrasound reading (reference angle: 45°) - (numeric: real)\n",
    "\t17. US17: ultrasound reading (reference angle: 60°) - (numeric: real)\n",
    "\t18. US18: ultrasound reading (reference angle: 75°) - (numeric: real)\n",
    "\t19. US19: ultrasound reading (reference angle: 90°) - (numeric: real)\n",
    "\t20. US20: ultrasound reading (reference angle: 105°) - (numeric: real)\n",
    "\t21. US21: ultrasound reading (reference angle: 120°) - (numeric: real)\n",
    "\t22. US22: ultrasound reading (reference angle: 135°) - (numeric: real)\n",
    "\t23. US23: ultrasound reading (reference angle: 150°) - (numeric: real)\n",
    "\t24. US24: ultrasound reading (reference angle: 165°) - (numeric: real)\n",
    "   \t25. Class: \n",
    "      \t\t-- Move-Forward\n",
    "      \t\t-- Slight-Right-Turn\n",
    "      \t\t-- Sharp-Right-Turn\n",
    "      \t\t-- Slight-Left-Turn\n",
    "\n",
    "   -- File sensor_readings_4.data:\n",
    "\t1. SD_front: minimum sensor reading within a 60 degree arc located at the front of the robot - (numeric: real)\n",
    "\t2. SD_left:  minimum sensor reading within a 60 degree arc located at the left of the robot  - (numeric: real)\n",
    "\t3. SD_right: minimum sensor reading within a 60 degree arc located at the right of the robot - (numeric: real)\n",
    "\t4. SD_back:  minimum sensor reading within a 60 degree arc located at the back of the robot - (numeric: real)\n",
    "   \t5. Class: \n",
    "      \t\t-- Move-Forward\n",
    "      \t\t-- Slight-Right-Turn\n",
    "      \t\t-- Sharp-Right-Turn\n",
    "      \t\t-- Slight-Left-Turn\n",
    "\n",
    "   -- File sensor_readings_2.data:\n",
    "\t1. SD_front: minimum sensor reading within a 60 degree arc located at the front of the robot - (numeric: real)\n",
    "\t2. SD_left:  minimum sensor reading within a 60 degree arc located at the left of the robot - (numeric: real)\n",
    "   \t3. Class: \n",
    "      \t\t-- Move-Forward\n",
    "      \t\t-- Slight-Right-Turn\n",
    "      \t\t-- Sharp-Right-Turn\n",
    "      \t\t-- Slight-Left-Turn\n",
    "\n",
    "   -- Summary Statistics:\n",
    "\t-- File sensor_readings_24.data:\n",
    "\t\t Max\t  Min\t  Mean\t    SD\t\t\n",
    "\t  US1\t5.0000\t0.40000\t 1.47162  0.80280                                        \t\t\t                     \n",
    "\t  US2 \t5.0250\t0.43700\t 2.32704  1.41015\n",
    "   \t  US3\t5.0290\t0.47000  2.48935  1.24743\n",
    "   \t  US4\t5.0170\t0.83300  2.79650  1.30937\n",
    "      \t  US5\t5.0000\t1.12000  2.95855  1.33922\n",
    "      \t  US6\t5.0050\t1.11400  2.89307  1.28258\n",
    "      \t  US7\t5.0080\t1.12200  3.35111  1.41369\n",
    "      \t  US8\t5.0870\t0.85900  2.54040  1.11155\n",
    "      \t  US9\t5.0000\t0.83600  3.12562  1.35697                                                          \n",
    "      \t  US10\t5.0220\t0.81000  2.83239  1.30784\n",
    "      \t  US11\t5.0190\t0.78300  2.54940  1.38203\n",
    "      \t  US12\t5.0000\t0.77800  2.07778  1.24930\n",
    "      \t  US13\t5.0030\t0.77000  2.12578  1.40717\n",
    "     \t  US14\t5.0000\t0.75600  2.19049  1.57687\n",
    "      \t  US15\t5.0000\t0.49500  2.20577  1.71543\n",
    "      \t  US16\t5.0000\t0.42400  1.20211  1.09857\n",
    "      \t  US17\t5.0000\t0.37300  0.98983  0.94207                                                          \n",
    "      \t  US18\t5.0000\t0.35400  0.91027  0.88953\n",
    "      \t  US19\t5.0000\t0.34000  1.05811  1.14463\n",
    "      \t  US20\t5.0000\t0.35500  1.07632  1.14150\n",
    "      \t  US21\t5.0000\t0.38000  1.01592  0.88744\n",
    "      \t  US22\t5.0000\t0.37000  1.77803  1.57169\n",
    "      \t  US23\t5.0000\t0.36700  1.55505  1.29145\n",
    "      \t  US24\t5.0000\t0.37700  1.57851  1.15048\n",
    "\n",
    "\t-- File sensor_readings_4.data:\n",
    "\t\t       Max    Min       Mean\t    SD\t\n",
    "\t   SD_front\t5   0.49500   1.29031\t  0.62670                     \n",
    "\t   SD_left\t5   0.34000   0.68127\t  0.34259\n",
    "\t   SD_right\t5   0.83600   1.88182\t  0.56253\n",
    "\t   SD_back\t5   0.36700   1.27369\t  0.82175\n",
    "\n",
    "\t-- File sensor_readings_2.data:\n",
    "\t\t       Max    Min       Mean\t    SD\t\n",
    "\t   SD_front\t5   0.49500   1.29031\t  0.62670                     \n",
    "\t   SD_left\t5   0.34000   0.68127\t  0.34259\n",
    "\n",
    " \n",
    "8. Missing Attribute Values: none\n",
    " \n",
    "9. Class Distribution: \n",
    "\t-- Move-Forward: 2205 samples (40.41%).\n",
    "      \t-- Slight-Right-Turn: 826 samples (15.13%).\n",
    "      \t-- Sharp-Right-Turn: 2097 samples (38.43%).\n",
    "      \t-- Slight-Left-Turn: 328 samples (6.01%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.Separate the label column and feature columns\n",
    "#### 2.One-hot encode the label data\n",
    "#### 3.Use the function below to normalize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will peerform min-max normalization on the specified column of a DataFrame\n",
    "def normalize_column(df, col_name):\n",
    "     df[col_name] = (df[col_name] - df[col_name].min())/(df[col_name].max()-df[col_name].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Separate the labels from data, label is the last column.\n",
    "\n",
    "#TODO: Create one-hot encoding for each string value in the labels\n",
    "\n",
    "#TODO: Perform normalization for each data column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separate the data for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Separate data for training and testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we can create the Deep Neural Network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "DNNmodel = Sequential()\n",
    "#TODO: Add several Dense layers with input shape same as data and output shape same as labels\n",
    "#      Choose relu as activation function for input and hidden layers\n",
    "#      Choose softmax activation function for the final layer\n",
    "\n",
    "#################---------            COMPILE THE MODEL         --------######################################\n",
    "DNNmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "DNNmodel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the DNNmodel on training data and evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Train the created model on training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "#TODO: Obtain the accuracy of the model for training and testing data\n",
    "#      To use metrics.accuracy_score obtain result np.argmax()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Created by Nicholas Stepanov: https://github.com/renowator*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
