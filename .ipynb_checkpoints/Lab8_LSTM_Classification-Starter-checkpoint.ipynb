{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Apprentice Lab 8 Starter Code\n",
    "#### Sentiment Classification for text\n",
    "\n",
    "1. Apply Stemmer\n",
    "2. Tokenize and retokenize\n",
    "3. Pad sequences\n",
    "4. Separate training and testing data\n",
    "5. Define Bidirectional LSTM model\n",
    "6. Train the LSTMmodel on training data\n",
    "7. Evaluate results\n",
    "\n",
    "Resources:\n",
    "\n",
    "https://keras.io/api/layers/core_layers/embedding/\n",
    "\n",
    "https://keras.io/api/layers/recurrent_layers/lstm/\n",
    "\n",
    "https://keras.io/api/layers/recurrent_layers/bidirectional/\n",
    "\n",
    "https://keras.io/api/preprocessing/text/#tokenizer-class\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#      IMPORT REQUIRED LIBRARIES\n",
    "import pandas\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import glob\n",
    "import os\n",
    "\n",
    "#       LOAD DATA INTO PYTHON\n",
    "datalist = []\n",
    "labellist = []\n",
    "train_path = \"Data/aclImdb/train/\"\n",
    "test_path = \"Data/aclImdb/test/\"\n",
    "\n",
    "for subdir, dir, files in os.walk(train_path):\n",
    "    if subdir == train_path:\n",
    "        continue\n",
    "    elif subdir == \"Data/aclImdb/train/neg\":\n",
    "        for text_path in glob.glob(subdir + \"/*.txt\"):\n",
    "            text_file = open(text_path, \"r\")\n",
    "            words = text_file.read().lower()\n",
    "            words = words.replace(',','')\n",
    "            words = words.replace('.','')\n",
    "            words = words.replace('?','')\n",
    "            words = words.replace('!','')\n",
    "            datalist.append(words)\n",
    "            labellist.append([0])\n",
    "    elif subdir == \"Data/aclImdb/train/pos\":\n",
    "        for text_path in glob.glob(subdir + \"/*.txt\"):\n",
    "            text_file = open(text_path, \"r\")\n",
    "            words = text_file.read().lower()\n",
    "            words = words.replace(',','')\n",
    "            words = words.replace('.','')\n",
    "            words = words.replace('?','')\n",
    "            words = words.replace('!','')\n",
    "            datalist.append(words)\n",
    "            labellist.append([1])\n",
    "datalist = pandas.DataFrame(datalist)\n",
    "labellist = pandas.DataFrame(labellist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's now load all the necessary objects from Keras in advance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     LOAD REQUIRED AI LIBRARIES OBJECTS\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,  Dropout, Flatten\n",
    "from keras.layers import LSTM, Conv1D, Input, MaxPooling1D, Bidirectional\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we apply the stemmer to our review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stem\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "#TODO: Apply the Stemmer to datalist sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we need to tokenize the words from the dataset and return numeric sequences that can be fed into a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize words\n",
    "max_nb_words = 100000\n",
    "tokenizer = Tokenizer(num_words=max_nb_words)\n",
    "#TODO: Apply Tokenizer to datalist stems\n",
    "\n",
    "# Retokenize\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "max_nb_words = len(word_index)\n",
    "#TODO: Retokenize with new amount of words\n",
    "\n",
    "#TODO: Pad obtained Sequences according to maximum post length\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The data is now ready, so we can split it and create the Bidirectional LSTM model|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Split Training and Testing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LSTM Sequential Model\n",
    "\n",
    "#Parameters\n",
    "batch_size = 128\n",
    "epochs = 3\n",
    "embedding_vecor_length = 32\n",
    "lstm_size = 16\n",
    "\n",
    "\n",
    "#Model\n",
    "model = Sequential()\n",
    "#TODO: Add Embedding layer with input size of sequence followed by a Dropout layer\n",
    "\n",
    "#TODO: Add a Bidirectional LSTM layer followed by a Dropout layer\n",
    "\n",
    "#TODO: Add a Dense layer with a single output neuron and sigmoid activation for binary prediction\n",
    "\n",
    "##############################################################################################################\n",
    "#################---------            COMPILE THE MODEL         --------######################################\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "#TODO: Train the Model on training data\n",
    "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split = 0.1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Once the model is trained we can evaluate on new data it never used for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Evaluate the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Created by Nicholas Stepanov: https://github.com/renowator*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
