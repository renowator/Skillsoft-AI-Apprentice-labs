{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Apprentice Lab 8 Solution\n",
    "#### Sentiment Classification for text\n",
    "\n",
    "1. Apply Stemmer\n",
    "2. Tokenize and retokenize\n",
    "3. Pad sequences\n",
    "4. Separate training and testing data\n",
    "5. Define Bidirectional LSTM model\n",
    "6. Train the LSTMmodel on training data\n",
    "7. Evaluate results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#      IMPORT REQUIRED LIBRARIES\n",
    "import pandas\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import glob\n",
    "import os\n",
    "\n",
    "#       LOAD DATA INTO PYTHON\n",
    "datalist = []\n",
    "labellist = []\n",
    "train_path = \"Data/aclImdb/train/\"\n",
    "test_path = \"Data/aclImdb/test/\"\n",
    "\n",
    "for subdir, dir, files in os.walk(train_path):\n",
    "    if subdir == train_path:\n",
    "        continue\n",
    "    elif subdir == \"Data/aclImdb/train/neg\":\n",
    "        for text_path in glob.glob(subdir + \"/*.txt\"):\n",
    "            text_file = open(text_path, \"r\")\n",
    "            words = text_file.read().lower()\n",
    "            words = words.replace(',','')\n",
    "            words = words.replace('.','')\n",
    "            words = words.replace('?','')\n",
    "            words = words.replace('!','')\n",
    "            datalist.append(words)\n",
    "            labellist.append([0])\n",
    "    elif subdir == \"Data/aclImdb/train/pos\":\n",
    "        for text_path in glob.glob(subdir + \"/*.txt\"):\n",
    "            text_file = open(text_path, \"r\")\n",
    "            words = text_file.read().lower()\n",
    "            words = words.replace(',','')\n",
    "            words = words.replace('.','')\n",
    "            words = words.replace('?','')\n",
    "            words = words.replace('!','')\n",
    "            datalist.append(words)\n",
    "            labellist.append([1])\n",
    "datalist = pandas.DataFrame(datalist)\n",
    "labellist = pandas.DataFrame(labellist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's now load all the necessary objects from Keras in advance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/renowator/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#     LOAD REQUIRED AI LIBRARIES OBJECTS\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,  Dropout, Flatten\n",
    "from keras.layers import LSTM, Conv1D, Input, MaxPooling1D, Bidirectional\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we apply the stemmer to our review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stem\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "#TODO: Apply the Stemmer to datalist sentences\n",
    "for i in (datalist):\n",
    "    datalist[i] = datalist[i].apply(lambda x: \" \".join(stemmer.stem(p) for p in x.split(\" \") if not x.isdigit()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we need to tokenize the words from the dataset and return numeric sequences that can be fed into a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 74034 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "#Tokenize words\n",
    "max_nb_words = 100000\n",
    "tokenizer = Tokenizer(num_words=max_nb_words)\n",
    "#TODO: Apply Tokenizer to datalist stems\n",
    "tokenizer.fit_on_texts(datalist[0])\n",
    "sequences = tokenizer.texts_to_sequences(datalist[0])\n",
    "# Retokenize\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "max_nb_words = len(word_index)\n",
    "#TODO: Retokenize with new amount of words\n",
    "tokenizer = Tokenizer(num_words=max_nb_words)\n",
    "tokenizer.fit_on_texts(datalist[0])\n",
    "sequences = tokenizer.texts_to_sequences(datalist[0])\n",
    "word_index = tokenizer.word_index\n",
    "#TODO: Pad obtained Sequences according to maximum post length\n",
    "max_post_len = np.max([ len(x) for x in sequences])\n",
    "sequences = sequence.pad_sequences(sequences, maxlen=max_post_len) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The data is now ready, so we can split it and create the Bidirectional LSTM model|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Split Training and Testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(sequences, labellist, test_size=0.25, stratify=labellist, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 2493, 32)          2369088   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2493, 32)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 32)                6272      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,375,393\n",
      "Trainable params: 2,375,393\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/renowator/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16875 samples, validate on 1875 samples\n",
      "Epoch 1/3\n",
      "16875/16875 [==============================] - 370s 22ms/step - loss: 0.5837 - accuracy: 0.6884 - val_loss: 0.4069 - val_accuracy: 0.8315\n",
      "Epoch 2/3\n",
      "16875/16875 [==============================] - 366s 22ms/step - loss: 0.3329 - accuracy: 0.8753 - val_loss: 0.3033 - val_accuracy: 0.8752\n",
      "Epoch 3/3\n",
      "16875/16875 [==============================] - 355s 21ms/step - loss: 0.2039 - accuracy: 0.9321 - val_loss: 0.2833 - val_accuracy: 0.8912\n"
     ]
    }
   ],
   "source": [
    "## LSTM Sequential Model\n",
    "\n",
    "#Parameters\n",
    "batch_size = 128\n",
    "epochs = 3\n",
    "embedding_vecor_length = 32\n",
    "lstm_size = 16\n",
    "\n",
    "\n",
    "#Model\n",
    "model = Sequential()\n",
    "#TODO: Add Embedding layer with input size of sequence followed by a Dropout layer\n",
    "model.add(Embedding(max_nb_words, embedding_vecor_length, input_length=max_post_len))\n",
    "model.add(Dropout(0.25))\n",
    "#TODO: Add a Bidirectional LSTM layer followed by a Dropout layer\n",
    "model.add(Bidirectional(LSTM(lstm_size)))\n",
    "model.add(Dropout(0.25))\n",
    "#TODO: Add a Dense layer with a single output neuron and sigmoid activation for binary prediction\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "##############################################################################################################\n",
    "#################---------            COMPILE THE MODEL         --------######################################\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "#TODO: Train the Model on training data\n",
    "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split = 0.1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Once the model is trained we can evaluate on new data it never used for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6250/6250 [==============================] - 39s 6ms/step\n",
      "0.8734400272369385\n"
     ]
    }
   ],
   "source": [
    "#TODO: Evaluate the model\n",
    "scores_ts = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(scores_ts[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Created by Nicholas Stepanov: https://github.com/renowator*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
