{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Apprentice Lab 6 Solution\n",
    "#### Convolutional Neural Network Classification\n",
    "\n",
    "1. Load and pre-process dataset using the provided pipeline\n",
    "2. Separate training and testing data\n",
    "3. Construct a Convolutional Neural Network model in Keras with specific input and output shapes\n",
    "4. Train the CNNmodel on training data\n",
    "5. Evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#      IMPORT REQUIRED LIBRARIES\n",
    "import pandas\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below is provided the pre-processing pipeline for CIFAR-10 dataset: https://en.wikipedia.org/wiki/CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### FUNCTIONS NEEDED FOR PREPROCESSING PIPELINE\n",
    "# This function performs a min-max normalization on a numpy array\n",
    "def normalize(x):\n",
    "    min_val = np.min(x)\n",
    "    max_val = np.max(x)\n",
    "    x = (x-min_val) / (max_val-min_val)\n",
    "    return x\n",
    "\n",
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "        argument\n",
    "            - x: a list of labels\n",
    "        return\n",
    "            - one hot encoding matrix (number of labels, number of class)\n",
    "    \"\"\"\n",
    "    encoded = np.zeros((len(x), 10))\n",
    "    \n",
    "    for idx, val in enumerate(x):\n",
    "        encoded[idx][val] = 1\n",
    "    \n",
    "    return encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################------------    DATA PREPROCESSING PIPELINE              ---------################\n",
    "##############################                       DO NOT MODIFY                              #################\n",
    "\n",
    "\n",
    "def load_cfar10_batch(cifar10_dataset_folder_path, batch_id):\n",
    "    with open(cifar10_dataset_folder_path + '/data_batch_' + str(batch_id), mode='rb') as file:\n",
    "        # note the encoding type is 'latin1'\n",
    "        batch = pickle.load(file, encoding='latin1')\n",
    "        \n",
    "    features = batch['data'].reshape((len(batch['data']), 3, 32, 32)).transpose(0, 2, 3, 1)\n",
    "    labels = batch['labels']\n",
    "        \n",
    "    return features, labels\n",
    "\n",
    "\n",
    "def _preprocess_and_save(normalize, one_hot_encode, features, labels, filename):\n",
    "    features = normalize(features)\n",
    "    labels = one_hot_encode(labels)\n",
    "\n",
    "    pickle.dump((features, labels), open(filename, 'wb'))\n",
    "\n",
    "\n",
    "def preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode):\n",
    "    n_batches = 5\n",
    "    valid_features = []\n",
    "    valid_labels = []\n",
    "\n",
    "    for batch_i in range(1, n_batches + 1):\n",
    "        features, labels = load_cfar10_batch(cifar10_dataset_folder_path, batch_i)\n",
    "        \n",
    "        # find index to be the point as validation data in the whole dataset of the batch (10%)\n",
    "        index_of_validation = int(len(features) * 0.1)\n",
    "\n",
    "        # preprocess the 90% of the whole dataset of the batch\n",
    "        # - normalize the features\n",
    "        # - one_hot_encode the lables\n",
    "        # - save in a new file named, \"preprocess_batch_\" + batch_number\n",
    "        # - each file for each batch\n",
    "        _preprocess_and_save(normalize, one_hot_encode,\n",
    "                             features[:-index_of_validation], labels[:-index_of_validation], \n",
    "                             'preprocess_batch_' + str(batch_i) + '.p')\n",
    "\n",
    "        # unlike the training dataset, validation dataset will be added through all batch dataset\n",
    "        # - take 10% of the whold dataset of the batch\n",
    "        # - add them into a list of\n",
    "        #   - valid_features\n",
    "        #   - valid_labels\n",
    "        valid_features.extend(features[-index_of_validation:])\n",
    "        valid_labels.extend(labels[-index_of_validation:])\n",
    "\n",
    "    # preprocess the all stacked validation dataset\n",
    "    _preprocess_and_save(normalize, one_hot_encode,\n",
    "                         np.array(valid_features), np.array(valid_labels),\n",
    "                         'preprocess_validation.p')\n",
    "\n",
    "    # load the test dataset\n",
    "    with open(cifar10_dataset_folder_path + '/test_batch', mode='rb') as file:\n",
    "        batch = pickle.load(file, encoding='latin1')\n",
    "\n",
    "    # preprocess the testing data\n",
    "    test_features = batch['data'].reshape((len(batch['data']), 3, 32, 32)).transpose(0, 2, 3, 1)\n",
    "    test_labels = batch['labels']\n",
    "\n",
    "    # Preprocess and Save all testing data\n",
    "    _preprocess_and_save(normalize, one_hot_encode,\n",
    "                         np.array(test_features), np.array(test_labels),\n",
    "                         'preprocess_training.p')\n",
    "    \n",
    "\n",
    "def load_preprocess_training_batch(batch_id):\n",
    "    \"\"\"\n",
    "    Load the Preprocessed Training data and return them in batches of <batch_size> or less\n",
    "    \"\"\"\n",
    "    filename = 'preprocess_batch_' + str(batch_id) + '.p'\n",
    "    features, labels = pickle.load(open(filename, mode='rb'))\n",
    "\n",
    "    # Return the training data in batches of size <batch_size> or less\n",
    "    return features, labels\n",
    "\n",
    "###############################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Process and save the supplied data using preprocess_and_save_data function defined above\n",
    "preprocess_and_save_data(\"Data/cifar-10-batches-py\",  normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's create a Convolutional Neural Network model for classiying images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/renowator/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 16, 16, 128)       1664      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 4, 4, 128)         65664     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 1, 1, 128)         65664     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 150,794\n",
      "Trainable params: 150,794\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#################---------  INSTANTIATE SEQUENTIAL THE MODEL  --------######################################\n",
    "import keras\n",
    "CNNmodel = keras.Sequential()\n",
    "############################################################################################################\n",
    "#TODO: Stack several Convolutional, Dropout and MaxPooling layers\n",
    "CNNmodel.add(keras.layers.Conv2D(128, kernel_size=(2, 2), strides=(2,2), activation='relu', input_shape=(32,32,3)))\n",
    "CNNmodel.add(keras.layers.Dropout(rate=0.05))\n",
    "CNNmodel.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "CNNmodel.add(keras.layers.Conv2D(128, kernel_size=(2, 2), strides=(2,2), activation='relu'))\n",
    "CNNmodel.add(keras.layers.Dropout(rate=0.05))\n",
    "CNNmodel.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "CNNmodel.add(keras.layers.Conv2D(128, kernel_size=(2, 2), strides=(2,2), activation='relu'))\n",
    "CNNmodel.add(keras.layers.Dropout(rate=0.05))\n",
    "#TODO: Flatten the image using Flatten layer and add several Dense layers\n",
    "#      The final layer should have the same amount of dimensions as labels and softmax activation\n",
    "CNNmodel.add(keras.layers.Flatten())\n",
    "CNNmodel.add(keras.layers.Dense(128, activation=\"relu\"))\n",
    "CNNmodel.add(keras.layers.Dropout(rate=0.2))\n",
    "CNNmodel.add(keras.layers.Dense(10, activation='softmax')) #Output layer\n",
    "##############################################################################################################\n",
    "#################---------            COMPILE THE MODEL         --------######################################\n",
    "CNNmodel.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "CNNmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#            LOAD THE PRE_PROCESSED DATA INTO PYTHON\n",
    "for i in range(1,5):\n",
    "    feat, lab = load_preprocess_training_batch(i)\n",
    "    if i == 1:\n",
    "        features = feat\n",
    "        labels = lab\n",
    "    else:\n",
    "        features = np.concatenate((features, feat))\n",
    "        labels = np.concatenate((labels, lab))\n",
    "#TODO: Separate training and testing data\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, labels, test_size = 0.3, random_state=25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now the data is loaded and we are ready for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25200 samples, validate on 10800 samples\n",
      "Epoch 1/50\n",
      "25200/25200 [==============================] - 16s 616us/step - loss: 1.8207 - accuracy: 0.3202 - val_loss: 1.5379 - val_accuracy: 0.4331\n",
      "Epoch 2/50\n",
      "25200/25200 [==============================] - 18s 730us/step - loss: 1.5109 - accuracy: 0.4451 - val_loss: 1.4166 - val_accuracy: 0.4895\n",
      "Epoch 3/50\n",
      "25200/25200 [==============================] - 25s 1ms/step - loss: 1.3930 - accuracy: 0.4948 - val_loss: 1.3411 - val_accuracy: 0.5094\n",
      "Epoch 4/50\n",
      "25200/25200 [==============================] - 24s 958us/step - loss: 1.3112 - accuracy: 0.5253 - val_loss: 1.2614 - val_accuracy: 0.5408\n",
      "Epoch 5/50\n",
      "25200/25200 [==============================] - 26s 1ms/step - loss: 1.2445 - accuracy: 0.5508 - val_loss: 1.2278 - val_accuracy: 0.5600\n",
      "Epoch 6/50\n",
      "25200/25200 [==============================] - 26s 1ms/step - loss: 1.1863 - accuracy: 0.5743 - val_loss: 1.2326 - val_accuracy: 0.5560\n",
      "Epoch 7/50\n",
      "25200/25200 [==============================] - 25s 1ms/step - loss: 1.1460 - accuracy: 0.5901 - val_loss: 1.2003 - val_accuracy: 0.5719\n",
      "Epoch 8/50\n",
      "25200/25200 [==============================] - 25s 1ms/step - loss: 1.1143 - accuracy: 0.6005 - val_loss: 1.2232 - val_accuracy: 0.5659\n",
      "Epoch 9/50\n",
      "25200/25200 [==============================] - 19s 768us/step - loss: 1.0785 - accuracy: 0.6124 - val_loss: 1.1912 - val_accuracy: 0.5758\n",
      "Epoch 10/50\n",
      "25200/25200 [==============================] - 15s 593us/step - loss: 1.0513 - accuracy: 0.6234 - val_loss: 1.1895 - val_accuracy: 0.5837\n",
      "Epoch 11/50\n",
      "25200/25200 [==============================] - 16s 628us/step - loss: 1.0215 - accuracy: 0.6334 - val_loss: 1.2149 - val_accuracy: 0.5693\n",
      "Epoch 12/50\n",
      "25200/25200 [==============================] - 26s 1ms/step - loss: 0.9905 - accuracy: 0.6456 - val_loss: 1.1777 - val_accuracy: 0.5882\n",
      "Epoch 13/50\n",
      "25200/25200 [==============================] - 26s 1ms/step - loss: 0.9706 - accuracy: 0.6492 - val_loss: 1.1676 - val_accuracy: 0.5884\n",
      "Epoch 14/50\n",
      "25200/25200 [==============================] - 27s 1ms/step - loss: 0.9411 - accuracy: 0.6631 - val_loss: 1.1983 - val_accuracy: 0.5867\n",
      "Epoch 15/50\n",
      "25200/25200 [==============================] - 27s 1ms/step - loss: 0.9263 - accuracy: 0.6677 - val_loss: 1.2033 - val_accuracy: 0.5846\n",
      "Epoch 16/50\n",
      "25200/25200 [==============================] - 29s 1ms/step - loss: 0.8985 - accuracy: 0.6785 - val_loss: 1.1813 - val_accuracy: 0.5906\n",
      "Epoch 17/50\n",
      "25200/25200 [==============================] - 27s 1ms/step - loss: 0.8842 - accuracy: 0.6857 - val_loss: 1.1763 - val_accuracy: 0.5927\n",
      "Epoch 18/50\n",
      "25200/25200 [==============================] - 25s 1ms/step - loss: 0.8605 - accuracy: 0.6911 - val_loss: 1.2085 - val_accuracy: 0.5930\n",
      "Epoch 19/50\n",
      "25200/25200 [==============================] - 26s 1ms/step - loss: 0.8539 - accuracy: 0.6956 - val_loss: 1.1835 - val_accuracy: 0.5960\n",
      "Epoch 20/50\n",
      "25200/25200 [==============================] - 28s 1ms/step - loss: 0.8324 - accuracy: 0.7010 - val_loss: 1.2330 - val_accuracy: 0.5871\n",
      "Epoch 21/50\n",
      "25200/25200 [==============================] - 26s 1ms/step - loss: 0.8188 - accuracy: 0.7002 - val_loss: 1.2166 - val_accuracy: 0.5940\n",
      "Epoch 22/50\n",
      "25200/25200 [==============================] - 27s 1ms/step - loss: 0.7956 - accuracy: 0.7134 - val_loss: 1.2471 - val_accuracy: 0.5881\n",
      "Epoch 23/50\n",
      "25200/25200 [==============================] - 28s 1ms/step - loss: 0.7792 - accuracy: 0.7182 - val_loss: 1.2156 - val_accuracy: 0.5909\n",
      "Epoch 24/50\n",
      "25200/25200 [==============================] - 27s 1ms/step - loss: 0.7645 - accuracy: 0.7231 - val_loss: 1.2983 - val_accuracy: 0.5797\n",
      "Epoch 25/50\n",
      "25200/25200 [==============================] - 28s 1ms/step - loss: 0.7580 - accuracy: 0.7257 - val_loss: 1.2486 - val_accuracy: 0.5917\n",
      "Epoch 26/50\n",
      "25200/25200 [==============================] - 28s 1ms/step - loss: 0.7380 - accuracy: 0.7311 - val_loss: 1.2692 - val_accuracy: 0.5895\n",
      "Epoch 27/50\n",
      "25200/25200 [==============================] - 20s 794us/step - loss: 0.7270 - accuracy: 0.7379 - val_loss: 1.2804 - val_accuracy: 0.5916\n",
      "Epoch 28/50\n",
      "25200/25200 [==============================] - 14s 561us/step - loss: 0.7194 - accuracy: 0.7396 - val_loss: 1.2967 - val_accuracy: 0.5806\n",
      "Epoch 29/50\n",
      "25200/25200 [==============================] - 19s 747us/step - loss: 0.7004 - accuracy: 0.7464 - val_loss: 1.2914 - val_accuracy: 0.5889\n",
      "Epoch 30/50\n",
      "25200/25200 [==============================] - 27s 1ms/step - loss: 0.6960 - accuracy: 0.7492 - val_loss: 1.2993 - val_accuracy: 0.5940\n",
      "Epoch 31/50\n",
      "25200/25200 [==============================] - 25s 1ms/step - loss: 0.6852 - accuracy: 0.7521 - val_loss: 1.3211 - val_accuracy: 0.5862\n",
      "Epoch 32/50\n",
      "25200/25200 [==============================] - 28s 1ms/step - loss: 0.6709 - accuracy: 0.7608 - val_loss: 1.3652 - val_accuracy: 0.5778\n",
      "Epoch 33/50\n",
      "25200/25200 [==============================] - 26s 1ms/step - loss: 0.6600 - accuracy: 0.7600 - val_loss: 1.3108 - val_accuracy: 0.5956\n",
      "Epoch 34/50\n",
      "25200/25200 [==============================] - 27s 1ms/step - loss: 0.6517 - accuracy: 0.7626 - val_loss: 1.3295 - val_accuracy: 0.5905\n",
      "Epoch 35/50\n",
      "25200/25200 [==============================] - 29s 1ms/step - loss: 0.6448 - accuracy: 0.7662 - val_loss: 1.3468 - val_accuracy: 0.5920\n",
      "Epoch 36/50\n",
      "25200/25200 [==============================] - 28s 1ms/step - loss: 0.6417 - accuracy: 0.7658 - val_loss: 1.3939 - val_accuracy: 0.5868\n",
      "Epoch 37/50\n",
      "25200/25200 [==============================] - 29s 1ms/step - loss: 0.6222 - accuracy: 0.7754 - val_loss: 1.3659 - val_accuracy: 0.5881\n",
      "Epoch 38/50\n",
      "25200/25200 [==============================] - 27s 1ms/step - loss: 0.6250 - accuracy: 0.7744 - val_loss: 1.3770 - val_accuracy: 0.5860\n",
      "Epoch 39/50\n",
      "25200/25200 [==============================] - 24s 960us/step - loss: 0.6177 - accuracy: 0.7739 - val_loss: 1.3837 - val_accuracy: 0.5926\n",
      "Epoch 40/50\n",
      "25200/25200 [==============================] - 27s 1ms/step - loss: 0.6127 - accuracy: 0.7787 - val_loss: 1.4580 - val_accuracy: 0.5735\n",
      "Epoch 41/50\n",
      "25200/25200 [==============================] - 26s 1ms/step - loss: 0.6007 - accuracy: 0.7819 - val_loss: 1.4434 - val_accuracy: 0.5889\n",
      "Epoch 42/50\n",
      "25200/25200 [==============================] - 27s 1ms/step - loss: 0.6045 - accuracy: 0.7804 - val_loss: 1.4501 - val_accuracy: 0.5884\n",
      "Epoch 43/50\n",
      "25200/25200 [==============================] - 28s 1ms/step - loss: 0.6037 - accuracy: 0.7800 - val_loss: 1.4080 - val_accuracy: 0.5872\n",
      "Epoch 44/50\n",
      "25200/25200 [==============================] - 27s 1ms/step - loss: 0.5856 - accuracy: 0.7860 - val_loss: 1.4246 - val_accuracy: 0.5884\n",
      "Epoch 45/50\n",
      "25200/25200 [==============================] - 28s 1ms/step - loss: 0.5782 - accuracy: 0.7925 - val_loss: 1.4526 - val_accuracy: 0.5885\n",
      "Epoch 46/50\n",
      "25200/25200 [==============================] - 26s 1ms/step - loss: 0.5788 - accuracy: 0.7931 - val_loss: 1.4649 - val_accuracy: 0.5829\n",
      "Epoch 47/50\n",
      "25200/25200 [==============================] - 27s 1ms/step - loss: 0.5665 - accuracy: 0.7957 - val_loss: 1.4686 - val_accuracy: 0.5852\n",
      "Epoch 48/50\n",
      "25200/25200 [==============================] - 27s 1ms/step - loss: 0.5744 - accuracy: 0.7893 - val_loss: 1.4652 - val_accuracy: 0.5872\n",
      "Epoch 49/50\n",
      "25200/25200 [==============================] - 26s 1ms/step - loss: 0.5640 - accuracy: 0.7930 - val_loss: 1.4913 - val_accuracy: 0.5858\n",
      "Epoch 50/50\n",
      "25200/25200 [==============================] - 27s 1ms/step - loss: 0.5560 - accuracy: 0.7998 - val_loss: 1.4659 - val_accuracy: 0.5821\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f3d4c55aef0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO: Train the model on training data, try to have as many epochs as possible\n",
    "CNNmodel.fit(x_train, y_train, batch_size=16, epochs=50, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's evaluate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.888968253968254\n",
      "\n",
      "\n",
      "Test Accuracy: 0.5821296296296297\n"
     ]
    }
   ],
   "source": [
    "#TODO: Obtain model accuracy on testing and training data\n",
    "from sklearn import metrics\n",
    "test_pred = CNNmodel.predict(x_test)\n",
    "train_pred = CNNmodel.predict(x_train)\n",
    "print(\"Train Accuracy:\", metrics.accuracy_score(np.argmax(np.array(y_train), axis=1), np.argmax(np.array(train_pred), axis=1)))\n",
    "print(\"\\n\\nTest Accuracy:\", metrics.accuracy_score(np.argmax(np.array(y_test), axis=1), np.argmax(test_pred,axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Created by Nicholas Stepanov: https://github.com/renowator*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
